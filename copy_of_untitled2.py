# -*- coding: utf-8 -*-
"""Copy of Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G5j3sJYmiSnUDTKSu0a2R0Sd7Yw1egMd
"""

# Install required packages with proper versions
!pip install -q -U transformers peft accelerate datasets bitsandbytes trl

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig,
    Trainer  # Import Trainer class
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import json
import os

# Configuration
os.environ["WANDB_DISABLED"] = "true"
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
data_path = "/content/qaparis.json"

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load model with quantization
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Set pad token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)
model = prepare_model_for_kbit_training(model)

# Improved LoRA config
peft_config = LoraConfig(
    r=16,  # Increased from 8
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,  # Increased from 0.05
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# Load and prepare data
with open(data_path) as f:
    data = json.load(f)

def format_instruction(sample):
    return f"<|system|>\nYou are a CLI expert assistant.</s>\n<|user|>\n{sample['question']}</s>\n<|assistant|>\n{sample['answer']}</s>"

dataset = Dataset.from_list(data)
dataset = dataset.map(lambda x: {"text": format_instruction(x)})

def tokenize(sample):
    return tokenizer(
        sample["text"],
        truncation=True,
        max_length=512,
        padding="max_length"
    )

dataset = dataset.map(tokenize, batched=True)

# Remove the original columns after tokenization
dataset = dataset.remove_columns(['question', 'answer', 'text'])


# Improved training arguments
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,  # Increased from 4
    gradient_accumulation_steps=2,  # Reduced from 4
    learning_rate=1e-4,  # Reduced from 2e-4
    num_train_epochs=3,  # Increased from 1
    logging_steps=10,
    save_strategy="no",
    optim="paged_adamw_8bit",  # Better optimizer for 4-bit
    warmup_ratio=0.1,  # Add warmup
    lr_scheduler_type="cosine",  # Better learning rate schedule
    report_to="none",
    fp16=True,
    remove_unused_columns=False  # Keep this as False to control column removal manually
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

trainer.train()

# Save adapter
model.save_pretrained("lora_adapter")

!python agent.py "List all Python files in the current directory recursively."

# eval.py
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
from peft import PeftModel
# Import load_metric from the evaluate library instead of datasets
from evaluate import load as load_metric
import json
import torch # Import torch for bnb_config

# Initialize models
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# 4-bit quantization config - Re-define this for the evaluation phase
# to potentially load models with quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load the base model with quantization
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config, # Apply quantization to the base model as well
    device_map="auto"
)
base_pipe = pipeline("text-generation", model=base_model, tokenizer=tokenizer)

# Load the fine-tuned model - Ensure it's loaded after the base model if possible,
# or consider loading the adapter onto the base model directly
ft_model = PeftModel.from_pretrained(base_model, "lora_adapter")
ft_pipe = pipeline("text-generation", model=ft_model, tokenizer=tokenizer)

# Test prompts
test_prompts = [
    "Create a new Git branch and switch to it.",
    "Compress the folder reports into reports.tar.gz.",
    "List all Python files in the current directory recursively.",
    "Set up a virtual environment and install requests.",
    "Fetch only the first ten lines of a file named output.log.",
    # Edge cases
    "Show disk usage for all mounted filesystems in human-readable format.",
    "Kill all processes containing 'python' in their name."
]

# Evaluation function
def evaluate(prompt, model_pipe, model_name):
    response = model_pipe(
        prompt,
        max_new_tokens=100,
        do_sample=False
    )[0]["generated_text"]
    return response[len(prompt):].strip()

# Compute metrics
# Ensure the evaluate library is installed
!pip install -q evaluate
bleu = load_metric("bleu")
rouge = load_metric("rouge")

results = []
for prompt in test_prompts:
    # You might need to manage model loading and unloading more explicitly
    # or ensure that the models are loaded sequentially if memory is tight.
    # For simplicity, we assume loading both quantized models might fit now.

    base_output = evaluate(prompt, base_pipe, "base")
    ft_output = evaluate(prompt, ft_pipe, "fine-tuned")

    # Calculate scores
    bleu_score = bleu.compute(
        predictions=[ft_output.split()],
        references=[[base_output.split()]]
    )["bleu"]

    rouge_score = rouge.compute(
        predictions=[ft_output],
        references=[base_output]
    )["rougeL"].mid

    # Manual quality scoring (0-2)
    quality_score = 0
    if any(c in ft_output for c in ["git", "tar", "find", "python", "head"]):
        quality_score = 1
    if any(ft_output.startswith(cmd) for cmd in ["git", "tar", "find"]):
        quality_score = 2

    results.append({
        "prompt": prompt,
        "base_output": base_output,
        "ft_output": ft_output,
        "bleu": bleu_score,
        "rougeL": rouge_score.fmeasure,
        "quality_score": quality_score
    })

# Save results
with open("eval_static.md", "w") as f:
    f.write("# Static Evaluation Results\n\n")
    f.write("| Prompt | Base Output | Fine-Tuned Output | BLEU | ROUGE-L | Quality (0-2) |\n")
    f.write("|--------|-------------|-------------------|------|---------|---------------|\n")
    for r in results:
        f.write(f"| `{r['prompt']}` | `{r['base_output']}` | `{r['ft_output']}` | {r['bleu']:.2f} | {r['rougeL']:.2f} | {r['quality_score']} |\n")

with open("eval_dynamic.md", "w") as f:
    f.write("# Dynamic Evaluation Results\n\n")
    f.write("| Prompt | Executed Command | Quality (0-2) |\n")
    f.write("|--------|------------------|---------------|\n")
    for r in results[:5]:  # Only official test prompts
        cmd = r['ft_output'].split('\n')[0] if '\n' in r['ft_output'] else r['ft_output']
        f.write(f"| `{r['prompt']}` | `{cmd}` | {r['quality_score']} |\n")