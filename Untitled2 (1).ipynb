{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61ee5b0df53245a39ae6c2dc4091ecb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ebf39cbdfde4f3db3aa046ccfb14311",
              "IPY_MODEL_de8f87b2984c49dbbd13d28f9b3f1d56",
              "IPY_MODEL_4580b9d7fde54d37b4eb7d29f9db9f3f"
            ],
            "layout": "IPY_MODEL_06e591e799974c84bc1f5640b185c887"
          }
        },
        "2ebf39cbdfde4f3db3aa046ccfb14311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2a92ab840f646b0a110b10ca660342d",
            "placeholder": "​",
            "style": "IPY_MODEL_e0705ca314974d048d8f6e59f8b2dcac",
            "value": "Map: 100%"
          }
        },
        "de8f87b2984c49dbbd13d28f9b3f1d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c935acec68994f7582fab5089128e25b",
            "max": 181,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08e24759975b4710922a9e3168f0933c",
            "value": 181
          }
        },
        "4580b9d7fde54d37b4eb7d29f9db9f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0ec6d85fefa45068eeb2c4b233ebdc3",
            "placeholder": "​",
            "style": "IPY_MODEL_f440e968342e455fa124324a1e11fb3c",
            "value": " 181/181 [00:00&lt;00:00, 4990.95 examples/s]"
          }
        },
        "06e591e799974c84bc1f5640b185c887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2a92ab840f646b0a110b10ca660342d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0705ca314974d048d8f6e59f8b2dcac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c935acec68994f7582fab5089128e25b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08e24759975b4710922a9e3168f0933c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0ec6d85fefa45068eeb2c4b233ebdc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f440e968342e455fa124324a1e11fb3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffbe3bdec3084c388cbf80af9ddc5205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33cbed91f24442e6ba6bb6f1a27f7417",
              "IPY_MODEL_466880be8f974e3b80baf1285b0e9770",
              "IPY_MODEL_4d287cf6e2e84696822fdf67b87922a2"
            ],
            "layout": "IPY_MODEL_d4d207000c9b4459982a68e7842ef4d5"
          }
        },
        "33cbed91f24442e6ba6bb6f1a27f7417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2e664e75f5a4f22892e72852462e37a",
            "placeholder": "​",
            "style": "IPY_MODEL_2679931d2eac435f86ff0499511b9c8e",
            "value": "Map: 100%"
          }
        },
        "466880be8f974e3b80baf1285b0e9770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5a6f3744cdb4bc7b0fba601e18d5ae7",
            "max": 181,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdf9bb7c9eca43d1a75d421fc0b735a1",
            "value": 181
          }
        },
        "4d287cf6e2e84696822fdf67b87922a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895ee8fff41d415bb22e00d2a57a2512",
            "placeholder": "​",
            "style": "IPY_MODEL_d01ed002437f404eac6bfb10543f44a9",
            "value": " 181/181 [00:00&lt;00:00, 1888.58 examples/s]"
          }
        },
        "d4d207000c9b4459982a68e7842ef4d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2e664e75f5a4f22892e72852462e37a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2679931d2eac435f86ff0499511b9c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5a6f3744cdb4bc7b0fba601e18d5ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdf9bb7c9eca43d1a75d421fc0b735a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "895ee8fff41d415bb22e00d2a57a2512": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d01ed002437f404eac6bfb10543f44a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages with proper versions\n",
        "!pip install -q -U transformers peft accelerate datasets bitsandbytes trl"
      ],
      "metadata": {
        "id": "x9-ie5KpiBi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "61ee5b0df53245a39ae6c2dc4091ecb6",
            "2ebf39cbdfde4f3db3aa046ccfb14311",
            "de8f87b2984c49dbbd13d28f9b3f1d56",
            "4580b9d7fde54d37b4eb7d29f9db9f3f",
            "06e591e799974c84bc1f5640b185c887",
            "d2a92ab840f646b0a110b10ca660342d",
            "e0705ca314974d048d8f6e59f8b2dcac",
            "c935acec68994f7582fab5089128e25b",
            "08e24759975b4710922a9e3168f0933c",
            "f0ec6d85fefa45068eeb2c4b233ebdc3",
            "f440e968342e455fa124324a1e11fb3c",
            "ffbe3bdec3084c388cbf80af9ddc5205",
            "33cbed91f24442e6ba6bb6f1a27f7417",
            "466880be8f974e3b80baf1285b0e9770",
            "4d287cf6e2e84696822fdf67b87922a2",
            "d4d207000c9b4459982a68e7842ef4d5",
            "b2e664e75f5a4f22892e72852462e37a",
            "2679931d2eac435f86ff0499511b9c8e",
            "c5a6f3744cdb4bc7b0fba601e18d5ae7",
            "cdf9bb7c9eca43d1a75d421fc0b735a1",
            "895ee8fff41d415bb22e00d2a57a2512",
            "d01ed002437f404eac6bfb10543f44a9"
          ]
        },
        "id": "NfWgTtocgQf2",
        "outputId": "2b7f8114-6ceb-44b2-b5d0-6615720653e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/181 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61ee5b0df53245a39ae6c2dc4091ecb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/181 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffbe3bdec3084c388cbf80af9ddc5205"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 02:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.028400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.118300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.823300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer  # Import Trainer class\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "data_path = \"/content/qaparis.json\"\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model with quantization\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Improved LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=16,  # Increased from 8\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.1,  # Increased from 0.05\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Load and prepare data\n",
        "with open(data_path) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def format_instruction(sample):\n",
        "    return f\"<|system|>\\nYou are a CLI expert assistant.</s>\\n<|user|>\\n{sample['question']}</s>\\n<|assistant|>\\n{sample['answer']}</s>\"\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "dataset = dataset.map(lambda x: {\"text\": format_instruction(x)})\n",
        "\n",
        "def tokenize(sample):\n",
        "    return tokenizer(\n",
        "        sample[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Remove the original columns after tokenization\n",
        "dataset = dataset.remove_columns(['question', 'answer', 'text'])\n",
        "\n",
        "\n",
        "# Improved training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,  # Increased from 4\n",
        "    gradient_accumulation_steps=2,  # Reduced from 4\n",
        "    learning_rate=1e-4,  # Reduced from 2e-4\n",
        "    num_train_epochs=3,  # Increased from 1\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    optim=\"paged_adamw_8bit\",  # Better optimizer for 4-bit\n",
        "    warmup_ratio=0.1,  # Add warmup\n",
        "    lr_scheduler_type=\"cosine\",  # Better learning rate schedule\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False  # Keep this as False to control column removal manually\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save adapter\n",
        "model.save_pretrained(\"lora_adapter\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python agent.py \"List all Python files in the current directory recursively.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0Xm6lvBnw5i",
        "outputId": "662debbe-1922-4fb2-9a1a-7a304866896b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-19 10:16:17.081090: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750328177.111055   13607 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750328177.120497   13607 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "\u001b[1;33m[DRY RUN]\u001b[0m \u001b[1;36mpython -m find -r\u001b[0m\n",
            "\u001b[1;32mCommand:\u001b[0m python -m find -r\n",
            "\n",
            "Use:\n",
            "python -m find -r\n",
            "\n",
            "Explanation:\n",
            "-m: runs the command as a module (Python script)\n",
            "-r: recursively list all files in the current directory\n",
            "\n",
            "Example:\n",
            "$ python -m find -r\n",
            "```\n",
            ".\n",
            "├── __init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from peft import PeftModel\n",
        "import json\n",
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class EnhancedEvaluator:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        # Load models\n",
        "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.ft_model = PeftModel.from_pretrained(self.base_model, \"lora_adapter\")\n",
        "\n",
        "        # Create pipelines\n",
        "        self.base_pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.base_model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.ft_pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.ft_model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "        # Test prompts with multiple acceptable solutions\n",
        "        self.test_prompts = [\n",
        "            {\n",
        "                \"prompt\": \"Create a new Git branch and switch to it.\",\n",
        "                \"valid_commands\": [\n",
        "                    r\"git checkout -b \\w+\",\n",
        "                    r\"git branch \\w+ && git checkout \\w+\",\n",
        "                    r\"git switch -c \\w+\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"prompt\": \"Compress the folder reports into reports.tar.gz.\",\n",
        "                \"valid_commands\": [\n",
        "                    r\"tar -czf reports.tar.gz reports\",\n",
        "                    r\"tar -cvzf reports.tar.gz reports\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"prompt\": \"List all Python files in the current directory recursively.\",\n",
        "                \"valid_commands\": [\n",
        "                    r\"find . -name '\\*.py'\",\n",
        "                    r\"ls -R \\| grep .py\",\n",
        "                    r\"grep -r --include='\\*.py' '' .\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"prompt\": \"Set up a virtual environment and install requests.\",\n",
        "                \"valid_commands\": [\n",
        "                    r\"python -m venv venv && source venv/bin/activate && pip install requests\",\n",
        "                    r\"virtualenv venv && . venv/bin/activate && pip install requests\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"prompt\": \"Fetch only the first ten lines of a file named output.log.\",\n",
        "                \"valid_commands\": [\n",
        "                    r\"head -n 10 output.log\",\n",
        "                    r\"sed -n '1,10p' output.log\"\n",
        "                ]\n",
        "            },\n",
        "            # Edge cases\n",
        "            {\n",
        "                \"prompt\": \"Delete all files with .tmp extension in /tmp directory safely.\",\n",
        "                \"valid_commands\": [\n",
        "                    r\"find /tmp -name '\\*.tmp' -type f -delete\",\n",
        "                    r\"find /tmp -name '\\*.tmp' -exec rm {} \\+\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"prompt\": \"Find and replace text 'old_text' with 'new_text' in all .py files recursively.\",\n",
        "                \"valid_commands\": [\n",
        "                    r\"find . -name '\\*.py' -exec sed -i 's/old_text/new_text/g' {} \\+\",\n",
        "                    r\"grep -rl 'old_text' --include='\\*.py' . | xargs sed -i 's/old_text/new_text/g'\"\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    def extract_command(self, response):\n",
        "        \"\"\"Improved command extraction with regex patterns\"\"\"\n",
        "        # Look for code blocks\n",
        "        code_match = re.search(r'```(?:bash|sh)?\\n(.*?)\\n```', response, re.DOTALL)\n",
        "        if code_match:\n",
        "            commands = [c.strip() for c in code_match.group(1).split('\\n') if c.strip()]\n",
        "            if commands:\n",
        "                return commands[0]\n",
        "\n",
        "        # Look for command-like lines\n",
        "        command_pattern = r'^\\s*(?:git|tar|find|python|head|sed|rm)\\s+.*'\n",
        "        for line in response.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if re.match(command_pattern, line):\n",
        "                return line\n",
        "\n",
        "        return response.strip()\n",
        "\n",
        "    def generate_response(self, pipe, prompt):\n",
        "        \"\"\"Generate response with better prompt engineering\"\"\"\n",
        "        system_prompt = (\n",
        "            \"<|system|>\\nYou are a CLI expert assistant. Respond ONLY with the exact command needed \"\n",
        "            \"to complete the task. Do not include any explanations, comments, or examples.\\n</s>\\n\"\n",
        "        )\n",
        "        user_prompt = f\"<|user|>\\n{prompt}</s>\\n<|assistant|>\\n\"\n",
        "        full_prompt = system_prompt + user_prompt\n",
        "\n",
        "        try:\n",
        "            response = pipe(\n",
        "                full_prompt,\n",
        "                max_new_tokens=50,\n",
        "                do_sample=False,\n",
        "                temperature=0.01,\n",
        "                top_k=1\n",
        "            )[0][\"generated_text\"][len(full_prompt):].strip()\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    def command_is_valid(self, command, valid_patterns):\n",
        "        \"\"\"Check if command matches any valid pattern\"\"\"\n",
        "        if not command or \"Error\" in command:\n",
        "            return False\n",
        "        return any(re.match(pattern, command) for pattern in valid_patterns)\n",
        "\n",
        "    def score_plan_quality(self, command, valid_patterns):\n",
        "        \"\"\"Improved quality scoring (0-2)\"\"\"\n",
        "        if not command or \"Error\" in command:\n",
        "            return 0\n",
        "\n",
        "        # Score 2: Command matches a valid pattern exactly\n",
        "        if any(re.fullmatch(pattern, command) for pattern in valid_patterns):\n",
        "            return 2\n",
        "\n",
        "        # Score 1: Command contains key elements\n",
        "        key_terms = [\"git\", \"tar\", \"find\", \"python\", \"head\", \"sed\", \"rm\"]\n",
        "        if any(term in command for term in key_terms):\n",
        "            return 1\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Run complete evaluation\"\"\"\n",
        "        results = []\n",
        "\n",
        "        print(\"Running Enhanced Evaluation...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for test in self.test_prompts:\n",
        "            prompt = test[\"prompt\"]\n",
        "            valid_patterns = test[\"valid_commands\"]\n",
        "\n",
        "            print(f\"\\nTest: {prompt}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Generate responses\n",
        "            base_response = self.generate_response(self.base_pipe, prompt)\n",
        "            ft_response = self.generate_response(self.ft_pipe, prompt)\n",
        "\n",
        "            # Extract commands\n",
        "            base_cmd = self.extract_command(base_response)\n",
        "            ft_cmd = self.extract_command(ft_response)\n",
        "\n",
        "            # Calculate ROUGE-L\n",
        "            base_rouge = self.scorer.score(prompt, base_cmd)['rougeL'].fmeasure\n",
        "            ft_rouge = self.scorer.score(prompt, ft_cmd)['rougeL'].fmeasure\n",
        "\n",
        "            # Score quality\n",
        "            base_quality = self.score_plan_quality(base_cmd, valid_patterns)\n",
        "            ft_quality = self.score_plan_quality(ft_cmd, valid_patterns)\n",
        "\n",
        "            # Check validity\n",
        "            base_valid = self.command_is_valid(base_cmd, valid_patterns)\n",
        "            ft_valid = self.command_is_valid(ft_cmd, valid_patterns)\n",
        "\n",
        "            print(f\"Base response: {base_cmd}\")\n",
        "            print(f\"Fine-tuned:    {ft_cmd}\")\n",
        "            print(f\"Valid? Base: {base_valid}, Fine-tuned: {ft_valid}\")\n",
        "            print(f\"ROUGE-L: Base: {base_rouge:.3f}, Fine-tuned: {ft_rouge:.3f}\")\n",
        "            print(f\"Quality: Base: {base_quality}/2, Fine-tuned: {ft_quality}/2\")\n",
        "\n",
        "            results.append({\n",
        "                \"prompt\": prompt,\n",
        "                \"base_command\": base_cmd,\n",
        "                \"ft_command\": ft_cmd,\n",
        "                \"base_valid\": base_valid,\n",
        "                \"ft_valid\": ft_valid,\n",
        "                \"base_rouge\": base_rouge,\n",
        "                \"ft_rouge\": ft_rouge,\n",
        "                \"base_quality\": base_quality,\n",
        "                \"ft_quality\": ft_quality\n",
        "            })\n",
        "\n",
        "        # Save results\n",
        "        with open(\"evaluation_results.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "        # Generate summary\n",
        "        self.generate_summary(results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_summary(self, results):\n",
        "        \"\"\"Generate evaluation summary\"\"\"\n",
        "        avg_base_rouge = sum(r[\"base_rouge\"] for r in results) / len(results)\n",
        "        avg_ft_rouge = sum(r[\"ft_rouge\"] for r in results) / len(results)\n",
        "        avg_base_quality = sum(r[\"base_quality\"] for r in results) / len(results)\n",
        "        avg_ft_quality = sum(r[\"ft_quality\"] for r in results) / len(results)\n",
        "\n",
        "        base_valid_count = sum(1 for r in results if r[\"base_valid\"])\n",
        "        ft_valid_count = sum(1 for r in results if r[\"ft_valid\"])\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"EVALUATION SUMMARY\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Valid Commands:\")\n",
        "        print(f\"  Base Model: {base_valid_count}/{len(results)}\")\n",
        "        print(f\"  Fine-tuned: {ft_valid_count}/{len(results)}\")\n",
        "\n",
        "        print(f\"\\nAverage ROUGE-L Score:\")\n",
        "        print(f\"  Base Model: {avg_base_rouge:.3f}\")\n",
        "        print(f\"  Fine-tuned: {avg_ft_rouge:.3f}\")\n",
        "        print(f\"  Improvement: {((avg_ft_rouge - avg_base_rouge) / avg_base_rouge * 100):.1f}%\")\n",
        "\n",
        "        print(f\"\\nAverage Quality Score (0-2):\")\n",
        "        print(f\"  Base Model: {avg_base_quality:.1f}\")\n",
        "        print(f\"  Fine-tuned: {avg_ft_quality:.1f}\")\n",
        "        print(f\"  Improvement: {((avg_ft_quality - avg_base_quality) / (avg_base_quality + 0.001) * 100):.1f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluator = EnhancedEvaluator()\n",
        "    results = evaluator.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_rVojGC3xYA",
        "outputId": "28ccea6c-5aeb-4558-cc8f-7eef914abaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Enhanced Evaluation...\n",
            "==================================================\n",
            "\n",
            "Test: Create a new Git branch and switch to it.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base response: git checkout -b new_branch\n",
            "Fine-tuned:    git checkout -b new_branch\n",
            "Valid? Base: True, Fine-tuned: True\n",
            "ROUGE-L: Base: 0.286, Fine-tuned: 0.286\n",
            "Quality: Base: 2/2, Fine-tuned: 2/2\n",
            "\n",
            "Test: Compress the folder reports into reports.tar.gz.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base response: tar -czvf reports.tar.gz reports\n",
            "Fine-tuned:    tar -czvf reports.tar.gz reports\n",
            "Valid? Base: False, Fine-tuned: False\n",
            "ROUGE-L: Base: 0.429, Fine-tuned: 0.429\n",
            "Quality: Base: 1/2, Fine-tuned: 1/2\n",
            "\n",
            "Test: List all Python files in the current directory recursively.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base response: python -m find -r\n",
            "Fine-tuned:    python -m find -r\n",
            "Valid? Base: False, Fine-tuned: False\n",
            "ROUGE-L: Base: 0.154, Fine-tuned: 0.154\n",
            "Quality: Base: 1/2, Fine-tuned: 1/2\n",
            "\n",
            "Test: Set up a virtual environment and install requests.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base response: $ virtualenv -p python3 env\n",
            "$ source env/bin/activate\n",
            "$ pip install requests\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "1. Set up a virtual environment.\n",
            "2. Activate the virtual environment.\n",
            "Fine-tuned:    $ virtualenv -p python3 env\n",
            "$ source env/bin/activate\n",
            "$ pip install requests\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "1. Set up a virtual environment.\n",
            "2. Activate the virtual environment.\n",
            "Valid? Base: False, Fine-tuned: False\n",
            "ROUGE-L: Base: 0.323, Fine-tuned: 0.323\n",
            "Quality: Base: 1/2, Fine-tuned: 1/2\n",
            "\n",
            "Test: Fetch only the first ten lines of a file named output.log.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base response: cat output.log | head -10\n",
            "Fine-tuned:    cat output.log | head -10\n",
            "Valid? Base: False, Fine-tuned: False\n",
            "ROUGE-L: Base: 0.235, Fine-tuned: 0.235\n",
            "Quality: Base: 1/2, Fine-tuned: 1/2\n",
            "\n",
            "Test: Delete all files with .tmp extension in /tmp directory safely.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base response: Use: rm -rf /tmp/.tmp*\n",
            "\n",
            "Explanation:\n",
            "-r: recursive (delete all files and subdirectories)\n",
            "-f: force (delete even if file is not a directory)\n",
            "-i: interactive\n",
            "Fine-tuned:    Use: rm -rf /tmp/.tmp*\n",
            "\n",
            "Explanation:\n",
            "-r: recursive (delete all files and subdirectories)\n",
            "-f: force (delete even if file is not a directory)\n",
            "-i: interactive\n",
            "Valid? Base: False, Fine-tuned: False\n",
            "ROUGE-L: Base: 0.229, Fine-tuned: 0.229\n",
            "Quality: Base: 1/2, Fine-tuned: 1/2\n",
            "\n",
            "Test: Find and replace text 'old_text' with 'new_text' in all .py files recursively.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base response: Use: find. -type f -name '*.py' -exec sed -i \"s/old_text/new_text/g\" {} \\;\n",
            "\n",
            "Example:\n",
            "    find. -type f -name '*.py'\n",
            "Fine-tuned:    Use: find. -type f -name '*.py' -exec sed -i \"s/old_text/new_text/g\" {} \\;\n",
            "\n",
            "Example:\n",
            "    find. -type f -name '*.py'\n",
            "Valid? Base: False, Fine-tuned: False\n",
            "ROUGE-L: Base: 0.343, Fine-tuned: 0.343\n",
            "Quality: Base: 1/2, Fine-tuned: 1/2\n",
            "\n",
            "==================================================\n",
            "EVALUATION SUMMARY\n",
            "==================================================\n",
            "Valid Commands:\n",
            "  Base Model: 1/7\n",
            "  Fine-tuned: 1/7\n",
            "\n",
            "Average ROUGE-L Score:\n",
            "  Base Model: 0.285\n",
            "  Fine-tuned: 0.285\n",
            "  Improvement: 0.0%\n",
            "\n",
            "Average Quality Score (0-2):\n",
            "  Base Model: 1.1\n",
            "  Fine-tuned: 1.1\n",
            "  Improvement: 0.0%\n"
          ]
        }
      ]
    }
  ]
}